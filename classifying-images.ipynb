{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ã“scar Mirones y Pablo Menezo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using convnets with small datasets\n",
    "\n",
    "The first thing to do is download the following file https://lara.web.cern.ch/lara/train.zip in the jupyter terminal and uncompress it in the same folder as this notebook. \n",
    "\n",
    "To download another dataset from imagenet you can do it with the URL list of the images and using `wget -i`\n",
    "\n",
    "\n",
    "\n",
    "## Training a convnet from scratch\n",
    "\n",
    "Training an image classification model with very little data is a common situation you will find yourself in if you end up doing Computer Vision in a professional context.  \n",
    "\n",
    "Having \"few\" samples can mean anything from a few hundred to a few tens of thousands of images.  Let's illustrate a practical example here: let's focus on classifying images as \"dogs\" and \"cats\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of DEEP Learning in problems with few data\n",
    "\n",
    "You may have heard many times that Deep Learning only works when you have large amounts of data. This is partly true: one of the characteristics of Deep learning is that you can find interesting features from the training dataset itself, and this a priori is easier when you have many examples available, especially in the case of having input datasets with a high dimensionality, such as images.\n",
    "\n",
    "However, what constitutes a \"large\" dataset is relative. Specifically relative to the size and depth of the network we are trying to train. It is not possible to sand a convnet so that it becomes a complete problem with only a few dozen examples, but a few hundred can be enough if the model is well assembled (we will understand what \"well assembled\" means throughout the Deep Learning course).\n",
    "\n",
    "Since convnets learn local characteristics, invariant under translations, they are very efficient in terms of the number of images needed to carry out perceptual problems. So training a convnet from 0 with a not very large dataset can still lead to reasonable results as we will see here.\n",
    "\n",
    "But there is more: Deep Learning models are highly \"recyclable\". One can, for example, take an image classification problem and a trained speech-to-text converter on a very big dataset and then reuse it for solving a completely different problem only by adding some small modifications. More specifically, in the case of Computer Vision, many pre-trained models (usually trained on the ImageNet dataset) are made public so that one can download them and use them to create powerful Computer Vision models with very little data.\n",
    "\n",
    "But here we will just run a simple example.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos\n",
    "\n",
    "The cat vs dog dataset we use is not a Keras package. It was posted on Kaggle.com as part of a Computer Vision problem in late 2013, when ConvNets were not yet so popular. \n",
    "\n",
    "The images are medium resolution JPGEs. It looks like this:\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's no surprise that the 2013 Kaggle cat vs dog competition was won by ConvNets. The best were able to achieve up to 95% accuracy. In our example we are still far from this accuracy, but during the Deep Learning course we have learned how to approach this value using different methods to improve the performance of neural networks. It should be noted that in this example we are training on approximately only 10% of the data that was used for the contest. \n",
    "After downloading the dataset and decompressing it, we are going to create a new dataset containing three subsets: a training set containing 1000 images of each class, a validation set with 500 images of each class, and finally a test set with 500 images of each class.\n",
    "\n",
    "Here we have a few lines of code that make us this distribution automatically:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\usuario\\\\Desktop\\\\master data science\\\\machine_learning_i\\\\MachineLearningI-English-master'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'C:\\\\Users\\\\usuario\\\\Desktop\\\\master data science\\\\machine_learning_i\\\\MachineLearningI-English-master/train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'C:\\\\Users\\\\usuario\\\\Desktop\\\\master data science\\\\machine_learning_i\\\\MachineLearningI-English-master/cats_and_dogs_small'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training dog images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So effectively we have 2000 training images, 1000 validation images and 1000 test images. In each of these subsets there are the same number of examples from each class: this is what is called a balanced binary classification system, which means that our classification accuracy will be an adequate metric of the success of our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "In the above example we have built a small convnet to solve the problem of classifying handwritten numbers using the MNIST dataset, so we are already familiar with the terminology that keras uses. We are going to reuse the general structure we had in the previous example: our convnet will have a stack of alternate layers of `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "However, since we are dealing with larger images and a more complex problem, we will create our network accordingly: it will have one more layer of `Conv2D` + `MaxPooling2D`. This serves to increase the capacity of the network and to further reduce the size of the feature maps, so that they are not so huge when they reach the flattening step. We start using 150x150 input images (an arbitrary choice), and end up with feature maps that are 7x7 in size before the flattening layer.\n",
    "\n",
    "It is important to note that the depth of feature maps progressively increases as we move through the neural network (from 32 to 128) while the size of feature maps decreases (from 148x148 to 7x7). You will see this pattern in almost all convnets.\n",
    "\n",
    "As we are attacking a binary classification problem (dog or cat), we are going to finish the network with a single unit (a dense layer of size 1) and with a sigmoid activation. This unit will encode the probability that our network is looking at one class or another.\n",
    "\n",
    "The final look of the model should be as follows:\n",
    "\n",
    "\n",
    "![modelo_red_animales.png](https://github.com/laramaktub/MachineLearningI/blob/master/modelo_red_animales.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el paso de compilaciÃ³n utilizaremos el optimizador `RMSprop`(lr=1e-4). Como nuestra red termina con una Ãºnica unidad sigmoide, vamos a utilizar binary crossentropy como nuestra funciÃ³n de pÃ©rdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4),loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data preprocessing\n",
    "\n",
    "The images must be properly formatted as float tensors before they are given to the net. That's just what we're going to do here. Before we pre-process them, the images are JPEG files. The steps to be able to give them to our network are roughly as follows:\n",
    "\n",
    "* Read the files with the images.\n",
    "* Decode the content of the JPEG in a \"grid\" with the RGB of the pixels \n",
    "* Turn that \"grill\" into floatation devices\n",
    "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval as neural networks prefer to work with small values. \n",
    "\n",
    "All this may seem very complicated but thanks to Keras our life is much easier and we can count on your tools to take care of these steps automatically. Keras has a module with tools for image processing, which can be found in `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` that allows us to automatically convert images we have on the hard disk into pre-processed tensors. This is exactly what we'll be using next. To do this we can use the flow_from_directory to take the images directly from the folders that we previously generated. We give it as input the folders where the training (or validation) images are, the size of the images (target_size), the size of the batch we're going to use (we're going to start with 20) and as there are only two categories, we tell it that we're going to use binary_crossentropy (class_mode). When we run these commands we'll get the following, the total number of images and how many classes there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('cats_and_dogs_small/train',target_size=(150,150),batch_size=20,\n",
    "                                                    class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory('cats_and_dogs_small/test',target_size=(150,150),batch_size=20,\n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator()\n",
    "validation_generator = validation_datagen.flow_from_directory('cats_and_dogs_small/validation',target_size=(150,150),\n",
    "                                                              batch_size=20,class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these generators: it takes us to a batch of 150x150 RGB images (dimensions `(20, 150, 150, 3)`) and binary tags (dimension `(20,)`). 20 is the number of examples in each batch (what we call the batch size). The generator generates these batches indefinitely: it runs a loop endlessly through all the images we have in the folder. That's why we have to type `break` to break the loop at some point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the fit. In this case, as what we have is a generator, we use fit_generator. We are going to run 30 epochs and use the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 35s 709ms/step - loss: 6.6634 - acc: 0.5140\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 0.7694 - acc: 0.6090\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.5834 - acc: 0.6830\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.5053 - acc: 0.7510\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 0.3736 - acc: 0.8140\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 0.3228 - acc: 0.8500\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 0.2401 - acc: 0.9000\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 34s 685ms/step - loss: 0.2220 - acc: 0.9220\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.1511 - acc: 0.9470\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 34s 684ms/step - loss: 0.2168 - acc: 0.9400\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 34s 680ms/step - loss: 0.0981 - acc: 0.9700\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 34s 680ms/step - loss: 0.1006 - acc: 0.9750\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 34s 675ms/step - loss: 0.1170 - acc: 0.9570\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.0896 - acc: 0.9780\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 34s 680ms/step - loss: 0.1052 - acc: 0.9670\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 0.0475 - acc: 0.9840\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 0.0210 - acc: 0.9930\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 34s 687ms/step - loss: 0.1276 - acc: 0.9800\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.1205 - acc: 0.9830\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.1351 - acc: 0.9720\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 34s 686ms/step - loss: 0.0066 - acc: 0.9990\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.0053 - acc: 0.9980\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 34s 684ms/step - loss: 0.1067 - acc: 0.9890\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 34s 678ms/step - loss: 0.0448 - acc: 0.9850\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.0906 - acc: 0.9830\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 9.4723e-04 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 34s 684ms/step - loss: 0.0466 - acc: 0.9880\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.2906 - acc: 0.9790\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.2500 - acc: 0.9780\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(validation_generator, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's a nice idea to save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('net_images.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate our model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('net_images.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.3683419227600098, 0.6520000100135803]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to optimize the network with the tools you have learnt during the lesson. Try to make improvements both in terms of speed and accuracy. Comment the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los mejores resultados lo hemos conseguido con esta arquitectura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model3.add(layers.MaxPooling2D((2,2)))\n",
    "model3.add(layers.Conv2D(64,(3,3),activation = 'relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "model3.add(layers.MaxPooling2D((2,2)))\n",
    "model3.add(layers.Conv2D(128,(3,3),activation = 'relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(layers.MaxPooling2D((2,2)))\n",
    "model3.add(layers.Conv2D(128,(3,3),activation = 'relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "model3.add(layers.MaxPooling2D((2,2)))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(512,activation = 'relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "model3.add(Dense(1, activation = 'sigmoid'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer=optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "                       loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "64/64 [==============================] - 26s 407ms/step - loss: 5.6684 - acc: 0.5195\n",
      "Epoch 2/30\n",
      "64/64 [==============================] - 26s 410ms/step - loss: 2.0654 - acc: 0.5898\n",
      "Epoch 3/30\n",
      "64/64 [==============================] - 27s 419ms/step - loss: 1.8594 - acc: 0.6352\n",
      "Epoch 4/30\n",
      "64/64 [==============================] - 27s 420ms/step - loss: 1.6995 - acc: 0.6695\n",
      "Epoch 5/30\n",
      "64/64 [==============================] - 27s 419ms/step - loss: 1.6015 - acc: 0.7000\n",
      "Epoch 6/30\n",
      "64/64 [==============================] - 27s 421ms/step - loss: 1.6143 - acc: 0.6828\n",
      "Epoch 7/30\n",
      "64/64 [==============================] - 27s 424ms/step - loss: 1.5169 - acc: 0.7258\n",
      "Epoch 8/30\n",
      "64/64 [==============================] - 27s 422ms/step - loss: 1.4714 - acc: 0.7703\n",
      "Epoch 9/30\n",
      "64/64 [==============================] - 27s 423ms/step - loss: 1.4134 - acc: 0.7859\n",
      "Epoch 10/30\n",
      "64/64 [==============================] - 27s 423ms/step - loss: 1.4075 - acc: 0.7906\n",
      "Epoch 11/30\n",
      "64/64 [==============================] - 27s 423ms/step - loss: 1.3759 - acc: 0.7883\n",
      "Epoch 12/30\n",
      "64/64 [==============================] - 27s 424ms/step - loss: 1.2912 - acc: 0.8367\n",
      "Epoch 13/30\n",
      "64/64 [==============================] - 27s 423ms/step - loss: 1.2881 - acc: 0.8562\n",
      "Epoch 14/30\n",
      "64/64 [==============================] - 27s 425ms/step - loss: 1.2677 - acc: 0.8562\n",
      "Epoch 15/30\n",
      "64/64 [==============================] - 27s 424ms/step - loss: 1.2433 - acc: 0.8570\n",
      "Epoch 16/30\n",
      "64/64 [==============================] - 27s 425ms/step - loss: 1.2099 - acc: 0.8773\n",
      "Epoch 17/30\n",
      "64/64 [==============================] - 27s 426ms/step - loss: 1.1812 - acc: 0.8938\n",
      "Epoch 18/30\n",
      "64/64 [==============================] - 27s 425ms/step - loss: 1.1514 - acc: 0.9039\n",
      "Epoch 19/30\n",
      "64/64 [==============================] - 27s 425ms/step - loss: 1.1297 - acc: 0.9070\n",
      "Epoch 20/30\n",
      "64/64 [==============================] - 27s 425ms/step - loss: 1.1050 - acc: 0.9148\n",
      "Epoch 21/30\n",
      "64/64 [==============================] - 28s 431ms/step - loss: 1.0525 - acc: 0.9359\n",
      "Epoch 22/30\n",
      "64/64 [==============================] - 28s 432ms/step - loss: 1.0314 - acc: 0.9445\n",
      "Epoch 23/30\n",
      "64/64 [==============================] - 28s 441ms/step - loss: 1.0284 - acc: 0.9453\n",
      "Epoch 24/30\n",
      "64/64 [==============================] - 28s 444ms/step - loss: 1.0299 - acc: 0.9367\n",
      "Epoch 25/30\n",
      "64/64 [==============================] - 27s 428ms/step - loss: 1.0209 - acc: 0.9398\n",
      "Epoch 26/30\n",
      "64/64 [==============================] - 28s 430ms/step - loss: 1.0040 - acc: 0.9492\n",
      "Epoch 27/30\n",
      "64/64 [==============================] - 28s 431ms/step - loss: 1.0001 - acc: 0.9469\n",
      "Epoch 28/30\n",
      "64/64 [==============================] - 28s 431ms/step - loss: 0.9668 - acc: 0.9609\n",
      "Epoch 29/30\n",
      "64/64 [==============================] - 28s 430ms/step - loss: 0.9580 - acc: 0.9617\n",
      "Epoch 30/30\n",
      "64/64 [==============================] - 28s 433ms/step - loss: 0.9343 - acc: 0.9688\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit_generator(train_generator, epochs=30,steps_per_epoch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6442365646362305, 0.722000002861023]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('net_images2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios generales:\n",
    "\n",
    "Cuando corrimos el primer modelo observamos que tenÃ­amos un acierto muy cercano a 1 a la hora de entrenar. Sin embargo, para el testeo se observÃ³ que el accuracy era del 65%. Por tanto, se podÃ­a establecer que el problema era de varianza. Hemos corrido mÃºltiples modelos sin obtener grandes resultados, ya que solamente hemos conseguido aumentar la accuracy hasta un 72% y mejorar la velocidad, pasando de 60-70s (con train, con validacion es 30-40 s) por Ã©poca a 20-30 s por Ã©poca. Entonces barajamos tres posibilidades principales:\n",
    "+ MÃ¡s datos: Nos da un mejor resultado ligeramente mejor porque si entrenamos con train aumentamos un 70 de accuracy a 72.\n",
    "+ RegularizaciÃ³n: Es la tÃ©cnica con la que hemos obtenido mejores resultados. Hemos aplicado en todas las capas una regularizaciÃ³n l2 con lambda 0.001 y un Dropout en mitad de la parte convolucional de 0.5, para intentar simplificar la red y que algÃºn nodo se pueda sobrespecializar. Del mismo modo lo hemos ido variando, con reg l1, mÃ¡s capas de dropout, cambiando los parÃ¡metros de regularizaciÃ³n, meter spatialDropout ...\n",
    "+ Otra estructura de red: Hemos probado a hacer diferentes redes, con diferentes capas de convoluciÃ³n, con AVGPooling, redes mÃ¡s grandes, mÃ¡s pequeÃ±as... pero no hemos logrado obtener un resultado satisfactorio con ellas.\n",
    "\n",
    "Porr Ãºltimo hemos realizado cambios en el optimizador que nos han dado mejores resultados para el accuracy y la velocidad. Hemos decidido utilizar ADAM Optimization ya que combina SGD con Momentum y RMSprop y por lo general ADAM parece que tiene un buen funcionamiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
